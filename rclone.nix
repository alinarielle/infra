{config, lib, inputs, ...}: {
  options.multi-homed = with lib.types; {
    enable = lib.mkEnableOption "enable multi-homed";
    cacheDir = lib.mkOption {
      type = nullOr path;
      default = "/persist/cache/rclone";
      description = ''
        the directory to use as VFS cache, will be created using systemd.tmpfiles.settings.
        make sure to put this on a fast storage disk on which you want to cache your files.
        set to null to not cache any files (not recommended)
      '';
    };
    s3 = lib.mkOption { default = {}; type = attrsOf (submodule { options = {
      paths = lib.mkOption {
        default = {}; 
        type = listOf path;
        description = ''
          mount the following directories from it's mirror on the remote.
          the directories must already exist.
        '';
        example = [
          "/home/alina/docs"
          "/home/alina/src"
        ];
      };
      bucket = lib.mkOption {
        description = ''
          the name of your S3 bucket (+ the path to your files), with trailing slash
        '';
        type = str;
        example = "woof/";
      };
      user = lib.mkOption {
        description = "the user under which to mount the bucket";
        type = str;
      };
      readOnly = lib.mkEnableOption "make the mount read only";
      accessKeyFile = lib.mkOption { 
        type = path; 
        default = null;
        description = "a path to a file containing the access key of your S3 bucket";
      };
      secretAccessKeyFile = lib.mkOption { 
        type = path; 
        default = null;
        description = "a path to a file containing the secret access key of your S3 bucket";
      };
      encryptedRemotePassphrase = lib.mkOption { 
        type = path;
        default = null; 
        description = ''
          a path to a file containing the obscured passphrase generated by rclone for the encrypted
          remote
        '';
      };
      encryptedRemoteSalt = lib.mkOption { 
        type = path; 
        default = null; 
        description = ''
          a path to a file containing the obscured salt generated by rclone for the encrypted remote
        '';
      };
      provider = lib.mkOption {
        description = ''
          rclone requires you to specify the cloud provider you are using for your S3 bucket.
          use "Other" for tigris, b2 for backblaze etc. refer to https://rclone.org/s3/
        '';
        type = str;
        default = "Other";
      };
      type = lib.mkOption { 
        type = str; 
        default = "s3"; 
        description = "the type of your remote backend";
      };
      endpoint = lib.mkOption { 
        type = str; 
        default = null; 
        example = "https://t3.storage.dev";
        description = "a URL to the endpoint for your S3 bucket";
      };
    }; });
  };};

  imports = [
    inputs.home-manager.nixosModules.home-manager
  ];
  
  config = let
    cfg = config.multi-homed;
    vfsArgs =  {
      cache-dir = cfg.cacheDir; # use this directory as VFS cache
      vfs-cache-mode = "full"; # cache everything
      vfs-cache-max-age = "off"; # dont remove stale objects from cache
      #vfs-cache-max-size = "10G"; # objects in cache are allowed to have a maximum size of 10GB
      vfs-cache-min-free-space = "5G";  # make sure there is still 100GB of storage space available 
                                          # on the disk used for caching
      vfs-cache-poll-interval = "5s"; # poll for new files every 5 seconds ?
      vfs-fast-fingerprint = true; # dont query slow file metadata for faster syncs, is less accurate
      #buffer-size = ""; # ???
      #vfs-read-ahead = ""; # extra read ahead over --buffer-size when using cache-mode full
      vfs-read-chunk-size = "4M"; # size of read-ahead chunks
      vfs-read-chunk-size-limit = "off";  # if greater than --vfs-read-chunk-size, double the chunk size after 
                                          # each chunk read, until the limit is reached ('off' is unlimited)
      vfs-read-chunk-streams = 16; # number of concurrent read ahead chunk downloads (per file?)
      vfs-refresh = true; # refreshes the directory cache recursively in the background on start
      links = true; # enable support for symlinks
      vfs-links = true; # enable support for symlinks in the VFS caching layer
      transfers = 4; # the number of concurrent file uploads from cache
      #vfs-used-is-size = true; # may be very expensive due to excessive API calls for rclone size in S3 backends
      vfs-write-back = "1s"; # time to write back files from cache after they have been last used
      write-back-cache = true;  # makes kernel buffer writes before sending them to rclone 
                                # (without this, writethrough caching is used)
      allow-non-empty = true; # allow mounting over a non-empty directory
      allow-root = true; # allow access to root user
      allow-other = true;
      async-read = true; # use asyncronous reads
      attr-timeout = "1s"; # cache file attributes for 1 second in the kernel to avoid excessive callbacks to rclone
      default-permissions = true; # makes kernel enforce access control based on the file mode
      dir-cache-time = "5m0s"; # time to cache directories for
      poll-interval = "5s"; # time to wait between polling for changes, must be smaller than dir-cache-time
    };
    s3Args = {
      s3-access-key = "%d/accessKey";
      s3-secret-access-key = "%d/secretAccessKey";
      s3-directory-markers = true;
    };
    cryptArgs = {
      crypt-password = "%d/encryptedRemotePassphrase";
      crypt-password2 = "%d/encryptedRemoteSalt";
    };
  in {
    programsfuse.userAllowOther = true;

    systemd.tmpfiles.settings."vfsCache".${cfg.cacheDir}.d = {
      user = "root"; group = "root"; type = "d"; age = "-"; mode = "0755";
    };

    imports = [./tasks.nix];

    tasks = lib.mapAttrs (key: val: {
      description = "mount ${key} with rclone";
      exec = [
        pkgs.rclone
      ] ++ lib.cli.toGNUCommandLine 
        (vfsArgs // s3Args // {
          s3-endpoint = val.endpoint;
          s3-provider = val.provider;
        } // cryptArgs // {
          crypt-remote = with val; ":s3,endpoint='${endpoint}',provider='${provider}'";
        });
      extraConfig.serviceConfig.LoadCredential = [
        "accessKey:${val.accessKeyFile}"
        "secretAccessKey:${val.secretAccessKeyFile}"
        "encryptedRemotePassphrase:${val.encryptedRemotePassphrase}"
        "encryptedRemoteSalt:${val.encryptedRemoteSalt}"
      ];
    }) cfg.s3;

  } // lib.mkMerge (lib.mapAttrsToList (key: val: {
    home-manager.users.${val.user}.programs.rclone = {
      enable = true;
      remotes.${key} = {
        config = {
          inherit (val) provider endpoint type;
        };
        secrets = {
          secret_access_key = val.accessKeyFile;
          access_key_id = val.secretAccessKeyFile;
        };
      };
      remotes."${key}-crypt" = {
        config = {
          type = "crypt";
          remote = key;
        };
        secrets = {
          password = val.encryptedRemotePassphrase;
          password2 = val.encryptedRemoteSalt;
        };
        mounts = lib.listToAttrs 
          (map 
            (path: lib.nameValuePair
              "${key}:${val.bucket}${path}"
              {
                enable = true;
                mountPoint = path;
                options = vfsCommon // lib.optionalAttrs val.readOnly { read-only = true; };
              }
            )
            val.paths
          )
        ;
      };
    };
  }) cfg.s3);
}
# TODO generate performance metrics with iperf/fio and automatically adjust VFS caching accordingly

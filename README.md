# alina's NixOS flake
this is the NixOS configuration for my entire infrastructure

## Structure
```
flake
├── flake.nix		# entry point & general configuration of the flake
├── flake.lock		# version pinning for nixpkgs and other dependencies
├── hosts		    # config for specific hosts
│   ├── tracer		    # network proxy for rDNS & test environment
│   ├── flora		    # hypervisor
│   ├── terra		    # restic backup host
│   ├── snow		    # hypervisor & storage box
│   ├── choco		    # desktop: workstation
│   ├── lilium		    # desktop: laptop
│   └── iso		        # custom NixOS installer image
├── meta		    # higher level abstractions like profiles, groups & tasks
├── network		    # network generation modules
├── secrets		    # secrets generated by sops-nix
├── services		# specific services
├── users		    # specific users
├── boot            # boot loader, secure boot & initramfs
├── filesystem      # storage cluster fs & s3 bucket generation 
├── deployment      # scripts & config for faster roll-outs
├── kernel          # my custom hardened kernel and linux security modules
├── packages        # dividing my packages into categories
├── desktop         # different tiling window managers sharing common code 
├── LICENSE         # the legal commitments you make if you use this code
├── .justfile 		# shortcuts for flake related commands
├── .sops.yaml		# sops-nix layout
└── .envrc		    # direnv entry point for the flake devshell
```

## General concepts
### Local modules
Every file is a module that can be enabled by ```path.to.file.enable = true;``` and is 
disabled per default. This is achieved with a function called lib.mkLocalModule that is
defined in meta/mkLocalModule.nix, which is used by passing ```./.``` (a string
interpolation of the current path), a description of the module which is passed into the
lib.mkEnableOption call, and finally the configuration of the following module.
The function then parses the string interpolation of the position of the file in the
source path, splits it up and turns it into an attribute set for the enable option and
the lib.mkIf condition.
To add some syntactic sugar, I wrote a small function that is referenced by 
```lib.meta.enable``` that is defined in meta/enable.nix. It allows you to write less
boilerplate code, specifically the ubiquitous ```file.enable = true;``` definitions that
result from my model of turning everything into a module. It is used as follows:
```
{lib, config, ...}: with lib.meta; {
    l.network = enable ["speed" "bbr"];
}
```
### Profiles
Profiles are the only place next to host specific configurations where decisions are made,
so one can always trace back where something was decided from a singular top entry point.
They activate more specific modules, which may depend on other, less abstracted modules
and algorithms. Take the base module as an example, it activates modules which should be
available on every machine, like the root and alina user, systemd-boot or zram.

Profiles are defined in meta/profiles and are activated by the enable function in
host-specific configurations, which are defined in hosts/. The host configs are then
activated as nixosConfigurations at flake.nix level.
They can capture any kind of configuration in its highly abstracted characters and can
describe anything from VMs to hypervisors and hardening.

### Tasks
A task is basically a wrapper that sets up apparmor and a systemd service and timer with 
very hardened but also specific capabilites derived from an algorithm in a white-listed 
and enforcive manner. It comes with an option to write a shell script which is executed
by a dynamic systemd user. Alternatively it is possible to pass a string interpolation of a package which will be computed to the nix store path of its binary and executed after 
the system has been setup. The applications launched are restricted as far as possible to
only allow the actions they were originally meant to perform, in case an attacker breaches
the first layer and is able to execute code as the program or its user.

### Services
Services wrap around applications and act as a helper for writing less boiler plate.
It will take information defined in an option namespace as arguments and derive service
specific configuration, which then activates underlying modules and passes requested
resources to the service file, namely port and IP address allocation.
The abstraction handles Group placement (see next paragraph), secret management with
sops-nix, priority level (whether the VM containing the service or the service itself
should be held as a duplicate in hot-standby on another machine and perform a live
migration in case a host fails to send a heartbeat signal in 5 minutes),
SSL certificates, nginx configuration, firewall settings and DNS.
Service files in services/ contain application specific configuration.

### Groups
Groups can be thought of task groups, they utilize the PQ secure wireguard mesh module and
vlan bridges to determine which services and hosts should be able to talk to each other on
a peer to peer level or in a mesh. They also aid in documenting which services run on 
multiple hosts and need to keep in sync with each other. The group abstractions also
provides lower level nix functions, for example calculating whether something is part of 
a group or removing duplicate group entries.

### IPs and ports
To write the wireguard mesh module, among others, I needed to find a way to know how to
subnet IP space in nix at build time. In order to do that, I implemented subnetting in nix
which hands out specific private IPs when called or calculates the requested size of an IP
space. I also implemented a deterministic pseudo-random port allocator, which sets a lower
and upper limit for the port allocation space (per default 50000 - 65355 to avoid
collisions) and then takes an identifier (for example wg-${meshname}) which hash's letters
are replaced with numbers from 1-26 and modulo'd down to the desired port range.

### VMs
I wrote a module for quickly and deterministically configuring VMs, aiming to be able to 
re-use code as much as possible. In a nutshell you just type ```l.vm.${servicename}```, 
where the service name is the name of an attribute set containing an enable option in 
l.services, and thus the name of the .nix file in services/. It is possible to pass
extra configuration to it by placing it in ```l.vm.${name}.extraConfiguration```.
Pre-written VM configurations is in hosts/vms and are activated on demand by turning them
into local modules with their own respective enable option. 

## Security features
### Apparmor
I use Apparmor on hosts that use the hardened profile because SELinux is a mess.

### Kernel hardening
Because I slightly disagree with most approaches to a hardened kernel configuration, I'm
maintaining my own. It is located in kernel/hardened.nix and used when the hardened
profile is active.

### noexec mount
Another feature specific to the hardened profile is to mount every partition as noexec,
except for the nix store. This way I can ensure only binaries which are downloaded from
trusted sources and meant to be on the system are executed.

### Impermanence
A rather common concept for advanced nix users, impermanence, turns the root filesystem
into a tmpfs and places bind mounts from /persist to every state that should stay on the
system, otherwise it is only stored in RAM and wiped when rebooting. This forces me to
manage my system exclusively from my nix flake in a declarative way as much as possible
and only persist state, which is also backed up.

## Desktop
### /home layout
I use a novel approach for my /home layout. It also uses impermanence, meaning everything 
except my media/{books,videos,pictures,music}, docs/ (document folder which is synced and
processed into paperless-ngx) and src/ (projects) folders are wiped and reboot and only
act as a volatile working space, such as the downloads/ folder that is deleted and created
every time.
### DEs and TWMs
I wrote configurations for a bunch of window managers, including sway, niri and hyprland.

## A lot here is still WIP!
